{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python import\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Our custom module\n",
    "sys.path.append('../')\n",
    "import data_processing\n",
    "import models\n",
    "from evaluation import *\n",
    "import submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma proposition : \n",
    "Avoir un Notebook template associé à des paramètres par défaut. \n",
    "Ensuite pour chaque expérience on le duplique, et on change les valeurs des paramètres que l'on souhaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../configs/default_params.yaml') as default_params_file:\n",
    "      default_params = yaml.safe_load(default_params_file)\n",
    "params = default_params\n",
    "params\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'script_path': '../train/A_COMPLETER.py',\n",
       " 'batch_size': 2,\n",
       " 'epochs': 1000,\n",
       " 'data_augmentation': {'samplewise_center': False,\n",
       "  'samplewise_std_normalization': False,\n",
       "  'rotation_range': 0,\n",
       "  'width_shift_range': 0.1,\n",
       "  'height_shift_range': 0.1,\n",
       "  'horizontal_flip': True,\n",
       "  'vertical_flip': False,\n",
       "  'zoom_range': 0,\n",
       "  'shear_range': 0,\n",
       "  'channel_shift_range': 0,\n",
       "  'featurewise_center': False,\n",
       "  'zca_whitening': False}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changer de paramètres\n",
    "params[\"train\"][\"batch_size\"] = 2\n",
    "params[\"train\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export masks as .tiff files\n",
    "\n",
    "If it's the first time you run this notebook, you should uncomment the following cell and run it. It will read the masks from the .csv file and output them as .tiff files in a \"train_masks\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.preproc import create_masks_as_tiff, preprocess_images_and_masks\n",
    "\n",
    "# create_masks_as_tiff(data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.utils import get_training_datasets_and_dataloaders\n",
    "\n",
    "train_dataset, validation_dataset, train_dataloader, validation_dataloader, encoder = get_training_datasets_and_dataloaders(batch_size=params[\"train\"][\"batch_size\"] ,input_size=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.unet import UNet\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "MODEL = UNet(num_classes=1).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=lr)\n",
    "loss = nn.MSELoss()\n",
    "classif_loss = nn.CrossEntropyLoss()\n",
    "p_s_classif_loss = nn.MSELoss()\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0.], dtype=torch.float64) tensor(2.3438)\n"
     ]
    }
   ],
   "source": [
    "for X, organ, label, p_size in train_dataset:\n",
    "    print(organ, p_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3086, -0.1451, -0.3237,  0.0342, -0.0397],\n",
      "        [-0.1510, -0.3272,  0.0541, -0.2237,  0.0833]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([[-0.0929],\n",
      "        [ 0.1501]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  1%|▏         | 2/141 [00:25<25:43, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -5.3991,  -5.6756,  -5.4803,   5.5822,  -5.6317],\n",
      "        [-10.4860, -10.3977, -10.7434,  10.5955, -10.4006]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([[ 5.8589],\n",
      "        [11.5642]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/141 [00:26<30:30, 13.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\predicting_organs_and_p_size.ipynb Cell 13\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/predicting_organs_and_p_size.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss_list, classif_loss_list, p_s_classif_loss_list \u001b[39m=\u001b[39m main_train_batch1(model\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39mto(device), loss_fn\u001b[39m=\u001b[39mloss, o_classif_loss_fn\u001b[39m=\u001b[39mclassif_loss, p_s_classif_loss_fn\u001b[39m=\u001b[39mp_s_classif_loss, optimizer\u001b[39m=\u001b[39moptimizer, n_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, dataset\u001b[39m=\u001b[39mtrain_dataset, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/predicting_organs_and_p_size.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/predicting_organs_and_p_size.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss_list, classif_loss_list, p_s_classif_loss_list \u001b[39m=\u001b[39m main_train(model\u001b[39m=\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mto(device), loss_fn\u001b[39m=\u001b[39;49mloss, o_classif_loss_fn\u001b[39m=\u001b[39;49mclassif_loss, p_s_classif_loss_fn\u001b[39m=\u001b[39;49mp_s_classif_loss, optimizer\u001b[39m=\u001b[39;49moptimizer, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\..\\train\\train.py:26\u001b[0m, in \u001b[0;36mmain_train\u001b[1;34m(model, loss_fn, o_classif_loss_fn, p_s_classif_loss_fn, optimizer, n_epochs, dataloader, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m start_time_epoch \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     24\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfor\u001b[39;00m X, organ, y, pixel_size \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m     27\u001b[0m     \u001b[39m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     model_output, organ_output, p_s_output \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(organ_output, p_s_output)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\..\\data_processing\\dataset.py:46\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m organ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded_organs[\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_files[idx][:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m])]\n\u001b[0;32m     45\u001b[0m image_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_transform(image)\n\u001b[1;32m---> 46\u001b[0m label_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_transform(label)\n\u001b[0;32m     47\u001b[0m image_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_df[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_files[idx][:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m])][\u001b[39m\"\u001b[39m\u001b[39mimg_height\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m ratio \u001b[39m=\u001b[39m image_size \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_size  \u001b[39m#ex : 512/3000 -> bigger picture, smaller pixel ?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:152\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39m# backward compatibility\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train.train import main_train, main_train_batch1\n",
    "\n",
    "if params[\"train\"][\"batch_size\"] == 1:\n",
    "    loss_list, classif_loss_list, p_s_classif_loss_list = main_train_batch1(model=MODEL.to(device), loss_fn=loss, o_classif_loss_fn=classif_loss, p_s_classif_loss_fn=p_s_classif_loss, optimizer=optimizer, n_epochs=50, dataset=train_dataset, device=device)\n",
    "if params[\"train\"][\"batch_size\"] > 1:\n",
    "    loss_list, classif_loss_list, p_s_classif_loss_list = main_train(model=MODEL.to(device), loss_fn=loss, o_classif_loss_fn=classif_loss, p_s_classif_loss_fn=p_s_classif_loss, optimizer=optimizer, n_epochs=50, dataloader=train_dataloader, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\predicting_organs_and_p_size.ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/predicting_organs_and_p_size.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(loss_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organ Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(classif_loss_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Size Prediction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_s_classif_loss_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "torch.save(MODEL, f\"../model_save/save_{time.gmtime(time.time())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Use evaluate_model.ipynb with saved model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "To do with the kaggle notebook for submit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
