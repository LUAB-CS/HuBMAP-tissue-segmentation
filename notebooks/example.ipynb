{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AlixChazottes/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Basic python import\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Our custom module\n",
    "sys.path.append('../')\n",
    "import data_processing\n",
    "import models\n",
    "from evaluation import *\n",
    "import submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma proposition : \n",
    "Avoir un Notebook template associé à des paramètres par défaut. \n",
    "Ensuite pour chaque expérience on le duplique, et on change les valeurs des paramètres que l'on souhaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../configs/default_params.yaml') as default_params_file:\n",
    "      default_params = yaml.safe_load(default_params_file)\n",
    "params = default_params\n",
    "params\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'script_path': '../train/A_COMPLETER.py',\n",
       " 'batch_size': 1,\n",
       " 'epochs': 1000,\n",
       " 'data_augmentation': {'samplewise_center': False,\n",
       "  'samplewise_std_normalization': False,\n",
       "  'rotation_range': 0,\n",
       "  'width_shift_range': 0.1,\n",
       "  'height_shift_range': 0.1,\n",
       "  'horizontal_flip': True,\n",
       "  'vertical_flip': False,\n",
       "  'zoom_range': 0,\n",
       "  'shear_range': 0,\n",
       "  'channel_shift_range': 0,\n",
       "  'featurewise_center': False,\n",
       "  'zca_whitening': False}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changer de paramètres\n",
    "params[\"train\"][\"batch_size\"] = 1\n",
    "params[\"train\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export masks as .tiff files\n",
    "\n",
    "If it's the first time you run this notebook, you should uncomment the following cell and run it. It will read the masks from the .csv file and output them as .tiff files in a \"train_masks\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_processing.preproc import create_masks_as_tiff, preprocess_images_and_masks\n",
    "\n",
    "# create_masks_as_tiff(data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.utils import get_training_datasets_and_dataloaders\n",
    "\n",
    "train_dataset, validation_dataset, train_dataloader, validation_dataloader = get_training_datasets_and_dataloaders(batch_size=params[\"train\"][\"batch_size\"] ,input_size=1024)\n",
    "#image, label, seg = train_dataset[0]\n",
    "#print(image.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.unet import UNet\n",
    "from models.segformer import SegFormer\n",
    "\n",
    "MODEL = UNet(num_classes=1).to(device)\n",
    "\n",
    "segformer = SegFormer(\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=1,\n",
    ")\n",
    "#MODEL = segformer.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=lr)\n",
    "loss = nn.MSELoss()\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = torch.load('../model_save/save_09_04_2023_09_27_53.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281 [00:00<?, ?it/s]/Users/AlixChazottes/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  2%|▏         | 6/281 [03:24<2:35:57, 34.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     loss_list \u001b[39m=\u001b[39m main_train_batch1(model\u001b[39m=\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mto(device), loss_fn\u001b[39m=\u001b[39;49mloss, optimizer\u001b[39m=\u001b[39;49moptimizer, n_epochs\u001b[39m=\u001b[39;49mn_epochs, dataset\u001b[39m=\u001b[39;49mtrain_dataset, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m      8\u001b[0m     loss_list \u001b[39m=\u001b[39m main_train(model\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39mto(device), loss_fn\u001b[39m=\u001b[39mloss, optimizer\u001b[39m=\u001b[39moptimizer, n_epochs\u001b[39m=\u001b[39mn_epochs, dataloader\u001b[39m=\u001b[39mtrain_dataloader, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Desktop/DL NLP/HuBMAP-tissue-segmentation/notebooks/../train/train.py:69\u001b[0m, in \u001b[0;36mmain_train_batch1\u001b[0;34m(model, loss_fn, optimizer, n_epochs, dataset, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m X, organ, y \u001b[39min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m     67\u001b[0m     \u001b[39m# 1. Forward pass\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(X,\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     model_output \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     71\u001b[0m     \u001b[39m# 2. Calculate and accumulate loss\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(model_output,y\u001b[39m.\u001b[39mto(device)) \u001b[39m#model_output[:,1,:,:] correspond au masque de la classe 2 = la zone d'intérêt, la classe 1 correpsond ua background\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/DL NLP/HuBMAP-tissue-segmentation/notebooks/../models/unet.py:82\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m x3_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3_1(torch\u001b[39m.\u001b[39mcat([x3_0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup(x4_0)], \u001b[39m1\u001b[39m))\n\u001b[1;32m     81\u001b[0m x2_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2_2(torch\u001b[39m.\u001b[39mcat([x2_0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup(x3_1)], \u001b[39m1\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m x1_3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1_3(torch\u001b[39m.\u001b[39;49mcat([x1_0, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup(x2_2)], \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     83\u001b[0m x0_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv0_4(torch\u001b[39m.\u001b[39mcat([x0_0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup(x1_3)], \u001b[39m1\u001b[39m))\n\u001b[1;32m     85\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal(x0_4)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/DL NLP/HuBMAP-tissue-segmentation/notebooks/../models/unet.py:34\u001b[0m, in \u001b[0;36mVGGBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m---> 34\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     35\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     36\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dlnlpenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train.train import main_train, main_train_batch1\n",
    "\n",
    "n_epochs = 1 # 25\n",
    "\n",
    "if params[\"train\"][\"batch_size\"] == 1:\n",
    "    loss_list = main_train_batch1(model=MODEL.to(device), loss_fn=loss, optimizer=optimizer, n_epochs=n_epochs, dataset=train_dataset, device=device)\n",
    "if params[\"train\"][\"batch_size\"] > 1:\n",
    "    loss_list = main_train(model=MODEL.to(device), loss_fn=loss, optimizer=optimizer, n_epochs=n_epochs, dataloader=train_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_mask(image,mask,cmaps):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 32))\n",
    "    hybr = image[0, :, :]/2 + mask[0, :, :]\n",
    "\n",
    "    ax[0].imshow(image.T)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('IMAGE')\n",
    "    ax[1].imshow(hybr.T,cmap=cmaps)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('MASK ON IMAGE')\n",
    "    plt.show()\n",
    "\n",
    "image, _, mask = train_dataset[0]\n",
    "\n",
    "show_image_and_mask(image,mask,\"gray\")\n",
    "show_image_and_mask(image,MODEL(torch.unsqueeze(image, dim=0).to(device)).cpu().detach(),\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "torch.save(MODEL, f\"../model_save/save_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Running model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred = {\"val_images\":[], \"val_masks\":[], \"val_organs\":[], \"val_y_preds\":[]}\n",
    "# for k, (val_image, val_organ, val_mask) in enumerate(train_dl):\n",
    "#     print(k)\n",
    "#     val_pred['val_images'].append(val_image)\n",
    "#     val_pred['val_masks'].append(val_mask)\n",
    "#     val_pred['val_organs'].append(val_organ)\n",
    "\n",
    "#     gc.collect()\n",
    "#     print(torch.cuda.memory_allocated(0),\n",
    "#         torch.cuda.memory_reserved(0),\n",
    "#         torch.cuda.max_memory_reserved(0),)\n",
    "#     pred = MODEL(val_image.to(device))\n",
    "#     val_pred['val_y_preds'].append(pred.cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation images, masks and organs\n",
    "val_preds = {0:{\"val_images\":[], \"val_masks\":[], \"val_organs\":[], \"val_y_preds\":[]}} #Only 1 fold\n",
    "\n",
    "for l, batch in enumerate(validation_dataset):\n",
    "    (val_images, val_organs, val_masks) = batch\n",
    "    val_mask_preds = MODEL(torch.unsqueeze(val_images, dim=0).to(device)).cpu().detach()\n",
    "    #print(f'VAL_Y_PREDS shape: {val_mask_preds.shape}, VAL_Y_PREDS dtype: {val_mask_preds.dtype}')\n",
    "    #print(f'val_images shape: {val_images.shape}, val_masks shape: {val_masks.shape}, val_organs shape: {len(val_organs)}')\n",
    "    # Cast from Tensorflow to Numpy\n",
    "    val_preds[0]['val_images'].append(val_images.numpy())\n",
    "    val_preds[0]['val_masks'].append(val_masks.numpy().astype(np.uint8))\n",
    "    val_preds[0]['val_organs'].append(val_organs)\n",
    "    val_preds[0]['val_y_preds'].append(val_mask_preds[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print(torch.cuda.memory_allocated(0),\n",
    "      torch.cuda.memory_reserved(0),\n",
    "    torch.cuda.max_memory_reserved(0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import get_y_true_y_pred\n",
    "\n",
    "IoU_Folds = dict()\n",
    "for fold, v in val_preds.items():\n",
    "    IoUs, IoUsOrgans = get_y_true_y_pred(v)\n",
    "    IoU_Folds[fold] = {\n",
    "        'IoUs': IoUs,\n",
    "        'IoUsOrgans': IoUsOrgans,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import plot_iou_by_threshold\n",
    "\n",
    "# Global Mean Intersection over Union at Threshold\n",
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    v['threshold_best'] = plot_iou_by_threshold(v['IoUs'], f'all_{fold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    percentiles = [0.01, 0.05, 0.10, 0.25, 0.40, 0.50, 0.60, 0.75, 0.90, 0.95, 0.99]\n",
    "    s = v['IoUs'][v['threshold_best']]\n",
    "\n",
    "    display(pd.Series(s).describe(percentiles=percentiles).apply(lambda v: f'{v:.2f}').to_frame(name='Value').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    pd.Series(v['IoUs'][v['threshold_best']]).plot(kind='hist')\n",
    "    plt.title('IoU Distribution at Best Threshold', size=24)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Threshold', size=16)\n",
    "    plt.ylabel('Count', size=16)\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    plt.xlim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import plot_validation_predictions\n",
    "\n",
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    threshold_best = IoU_Folds[fold]['threshold_best']\n",
    "    plot_validation_predictions(val_preds[fold], threshold_best, 2, IoUs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import submission\n",
    "\n",
    "test_dataset = None # TO BUILD\n",
    "submission.make_submission(MODEL, test_dataset, threshold_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
