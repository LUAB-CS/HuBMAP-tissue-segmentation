{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bapti\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Basic python import\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Our custom module\n",
    "sys.path.append('../')\n",
    "import data_processing\n",
    "import models\n",
    "from evaluation import *\n",
    "import submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma proposition : \n",
    "Avoir un Notebook template associé à des paramètres par défaut. \n",
    "Ensuite pour chaque expérience on le duplique, et on change les valeurs des paramètres que l'on souhaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../configs/default_params.yaml') as default_params_file:\n",
    "      default_params = yaml.safe_load(default_params_file)\n",
    "params = default_params\n",
    "params\n",
    "\n",
    "data_dir = os.path.join('..','data')\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'script_path': '../train/A_COMPLETER.py',\n",
       " 'batch_size': 1,\n",
       " 'epochs': 1000,\n",
       " 'data_augmentation': {'samplewise_center': False,\n",
       "  'samplewise_std_normalization': False,\n",
       "  'rotation_range': 0,\n",
       "  'width_shift_range': 0.1,\n",
       "  'height_shift_range': 0.1,\n",
       "  'horizontal_flip': True,\n",
       "  'vertical_flip': False,\n",
       "  'zoom_range': 0,\n",
       "  'shear_range': 0,\n",
       "  'channel_shift_range': 0,\n",
       "  'featurewise_center': False,\n",
       "  'zca_whitening': False}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changer de paramètres\n",
    "params[\"train\"][\"batch_size\"] = 1\n",
    "params[\"train\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export masks as .tiff files\n",
    "\n",
    "If it's the first time you run this notebook, you should uncomment the following cell and run it. It will read the masks from the .csv file and output them as .tiff files in a \"train_masks\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_processing.preproc import create_masks_as_tiff, preprocess_images_and_masks\n",
    "\n",
    "# create_masks_as_tiff(data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.utils import get_training_datasets_and_dataloaders\n",
    "\n",
    "train_dataset, validation_dataset, train_dataloader, validation_dataloader = get_training_datasets_and_dataloaders(batch_size=params[\"train\"][\"batch_size\"] ,input_size=512)\n",
    "#image, label, seg = train_dataset[0]\n",
    "#print(image.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.unet import UNet\n",
    "from models.segformer import SegFormer\n",
    "\n",
    "MODEL = UNet(num_classes=1).to(device)\n",
    "\n",
    "segformer = SegFormer(\n",
    "    in_channels=3,\n",
    "    widths=[64, 128, 256, 512],\n",
    "    depths=[3, 4, 6, 3],\n",
    "    all_num_heads=[1, 2, 4, 8],\n",
    "    patch_sizes=[7, 3, 3, 3],\n",
    "    overlap_sizes=[4, 2, 2, 2],\n",
    "    reduction_ratios=[8, 4, 2, 1],\n",
    "    mlp_expansions=[4, 4, 4, 4],\n",
    "    decoder_channels=256,\n",
    "    scale_factors=[8, 4, 2, 1],\n",
    "    num_classes=1,\n",
    ")\n",
    "#MODEL = segformer.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=lr)\n",
    "loss = nn.MSELoss()\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = torch.load('../model_save/save_09_04_2023_09_27_53.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281 [00:00<?, ?it/s]c:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 1024, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 0/281 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 4.00 GiB total capacity; 3.32 GiB already allocated; 0 bytes free; 3.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\example.ipynb Cell 13\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/example.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m# 25\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/example.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/example.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss_list \u001b[39m=\u001b[39m main_train_batch1(model\u001b[39m=\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mto(device), loss_fn\u001b[39m=\u001b[39;49mloss, optimizer\u001b[39m=\u001b[39;49moptimizer, n_epochs\u001b[39m=\u001b[39;49mn_epochs, dataset\u001b[39m=\u001b[39;49mtrain_dataset, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/example.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bapti/Documents/Cours3A/DL/HuBMAP-tissue-segmentation/notebooks/example.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss_list \u001b[39m=\u001b[39m main_train(model\u001b[39m=\u001b[39mMODEL\u001b[39m.\u001b[39mto(device), loss_fn\u001b[39m=\u001b[39mloss, optimizer\u001b[39m=\u001b[39moptimizer, n_epochs\u001b[39m=\u001b[39mn_epochs, dataloader\u001b[39m=\u001b[39mtrain_dataloader, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\Documents\\Cours3A\\DL\\HuBMAP-tissue-segmentation\\notebooks\\..\\train\\train.py:84\u001b[0m, in \u001b[0;36mmain_train_batch1\u001b[1;34m(model, loss_fn, optimizer, n_epochs, dataset, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[39m# 4. Loss backward\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     86\u001b[0m \u001b[39m# 5. Optimizer step\u001b[39;00m\n\u001b[0;32m     87\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bapti\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 4.00 GiB total capacity; 3.32 GiB already allocated; 0 bytes free; 3.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from train.train import main_train, main_train_batch1\n",
    "\n",
    "n_epochs = 100 # 25\n",
    "\n",
    "if params[\"train\"][\"batch_size\"] == 1:\n",
    "    loss_list = main_train_batch1(model=MODEL.to(device), loss_fn=loss, optimizer=optimizer, n_epochs=n_epochs, dataset=train_dataset, device=device)\n",
    "if params[\"train\"][\"batch_size\"] > 1:\n",
    "    loss_list = main_train(model=MODEL.to(device), loss_fn=loss, optimizer=optimizer, n_epochs=n_epochs, dataloader=train_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_mask(image,mask,cmaps):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 32))\n",
    "    hybr = image[0, :, :]/2 + mask[0, :, :]\n",
    "\n",
    "    ax[0].imshow(image.T)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('IMAGE')\n",
    "    ax[1].imshow(hybr.T,cmap=cmaps)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('MASK ON IMAGE')\n",
    "    plt.show()\n",
    "\n",
    "image, _, mask = train_dataset[0]\n",
    "\n",
    "show_image_and_mask(image,mask,\"gray\")\n",
    "show_image_and_mask(image,MODEL(torch.unsqueeze(image, dim=0).to(device)).cpu().detach(),\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "torch.save(MODEL, f\"../model_save/save_{datetime.now().strftime('%d_%m_%Y_%H_%M_%S')}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Running model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred = {\"val_images\":[], \"val_masks\":[], \"val_organs\":[], \"val_y_preds\":[]}\n",
    "# for k, (val_image, val_organ, val_mask) in enumerate(train_dl):\n",
    "#     print(k)\n",
    "#     val_pred['val_images'].append(val_image)\n",
    "#     val_pred['val_masks'].append(val_mask)\n",
    "#     val_pred['val_organs'].append(val_organ)\n",
    "\n",
    "#     gc.collect()\n",
    "#     print(torch.cuda.memory_allocated(0),\n",
    "#         torch.cuda.memory_reserved(0),\n",
    "#         torch.cuda.max_memory_reserved(0),)\n",
    "#     pred = MODEL(val_image.to(device))\n",
    "#     val_pred['val_y_preds'].append(pred.cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation images, masks and organs\n",
    "val_preds = {0:{\"val_images\":[], \"val_masks\":[], \"val_organs\":[], \"val_y_preds\":[]}} #Only 1 fold\n",
    "\n",
    "for l, batch in enumerate(validation_dataset):\n",
    "    (val_images, val_organs, val_masks) = batch\n",
    "    val_mask_preds = MODEL(torch.unsqueeze(val_images, dim=0).to(device)).cpu().detach()\n",
    "    #print(f'VAL_Y_PREDS shape: {val_mask_preds.shape}, VAL_Y_PREDS dtype: {val_mask_preds.dtype}')\n",
    "    #print(f'val_images shape: {val_images.shape}, val_masks shape: {val_masks.shape}, val_organs shape: {len(val_organs)}')\n",
    "    # Cast from Tensorflow to Numpy\n",
    "    val_preds[0]['val_images'].append(val_images.numpy())\n",
    "    val_preds[0]['val_masks'].append(val_masks.numpy().astype(np.uint8))\n",
    "    val_preds[0]['val_organs'].append(val_organs)\n",
    "    val_preds[0]['val_y_preds'].append(val_mask_preds[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print(torch.cuda.memory_allocated(0),\n",
    "      torch.cuda.memory_reserved(0),\n",
    "    torch.cuda.max_memory_reserved(0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import get_y_true_y_pred\n",
    "\n",
    "IoU_Folds = dict()\n",
    "for fold, v in val_preds.items():\n",
    "    IoUs, IoUsOrgans = get_y_true_y_pred(v)\n",
    "    IoU_Folds[fold] = {\n",
    "        'IoUs': IoUs,\n",
    "        'IoUsOrgans': IoUsOrgans,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import plot_iou_by_threshold\n",
    "\n",
    "# Global Mean Intersection over Union at Threshold\n",
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    v['threshold_best'] = plot_iou_by_threshold(v['IoUs'], f'all_{fold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    percentiles = [0.01, 0.05, 0.10, 0.25, 0.40, 0.50, 0.60, 0.75, 0.90, 0.95, 0.99]\n",
    "    s = v['IoUs'][v['threshold_best']]\n",
    "\n",
    "    display(pd.Series(s).describe(percentiles=percentiles).apply(lambda v: f'{v:.2f}').to_frame(name='Value').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    pd.Series(v['IoUs'][v['threshold_best']]).plot(kind='hist')\n",
    "    plt.title('IoU Distribution at Best Threshold', size=24)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Threshold', size=16)\n",
    "    plt.ylabel('Count', size=16)\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    plt.xlim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import plot_validation_predictions\n",
    "\n",
    "for fold, v in IoU_Folds.items():\n",
    "    print('=' * 80)\n",
    "    print(f'FOLD {fold}')\n",
    "    print('=' * 80)\n",
    "    threshold_best = IoU_Folds[fold]['threshold_best']\n",
    "    plot_validation_predictions(val_preds[fold], threshold_best, 2, IoUs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import submission\n",
    "\n",
    "test_dataset = None # TO BUILD\n",
    "submission.make_submission(MODEL, test_dataset, threshold_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
